{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Supervised ImageNet Training\n",
    "\n",
    "In this tutorial we will learn how to train a *supervised* model on the ImageNet dataset using our VISSL framework. Although, VISSL is focussed for self-supervised learning, it can be easily used to train supervised image classification models.\n",
    "\n",
    "VISSL relies on config files that can fully reproduce an experiment, and this tutorial will use the config provided at `hydra_configs/config/pretrain/supervised/supervised_1gpu_resnet_example.yaml`. This config trains a ResNet-50 model using a single GPU and we show how to extend this to multi-GPU or multi-node training.\n",
    "\n",
    "\n",
    "We first go over the requirements for training this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "This tutorial assumes you have access to a relatively modern machine running Linux with a GPU and enough disk space for the ImageNet images (~160 GB).\n",
    "\n",
    "\n",
    "1. Installing VISSL and its dependencies. VISSL can be easily installed using `conda` or `pip` as noted in `INSTALL.MD`.\n",
    "2. Download the ImageNet images and preprocess the `validation` set images using the [script](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh). This preprocessing is exactly the same as in the standard [PyTorch ImageNet example](https://github.com/pytorch/examples/tree/master/imagenet), and so it can be skipped if you already have the preprocessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training\n",
    "\n",
    "You will need to change the paths to the dataset in the config file `hydra_configs/config/pretrain/supervised/supervised_1gpu_resnet_example.yaml`. Specifically, you will need to provide the paths to the `train` and `val` folders of ImageNet.\n",
    "\n",
    "You can optionally change the output path by changing the string at `CHECKPOINT.DIR` in the config file. By default, the config will output the training log and the model checkpoints to a directory named `checkpoints`.\n",
    "\n",
    "\n",
    "Assuming you are in the root directory of the VISSL repo, the training can be started using the command \n",
    "```sh\n",
    "python tools/distributed_train.py config=pretrain/supervised/supervised_1gpu_resnet_example\n",
    "``` \n",
    "**Note** that the initial part of the config path, `hyrdra_configs/config` is omitted when running the training command.\n",
    "\n",
    "If all goes well, you should see the training script start with messages such as:\n",
    "\n",
    "```bash\n",
    "INFO 2020-07-01 14:06:06,196 distributed_train.py: 132: Spawning process for node_id: 0, local_rank: 0, dist_rank: 0, dist_run_id: 127.0.0.1:51899\n",
    "INFO 2020-07-01 14:06:06,197 train.py:  50: Env set for rank: 0, dist_rank: 0\n",
    "INFO 2020-07-01 14:06:06,198 env.py:  20: printing stats env\n",
    "```\n",
    "\n",
    "VISSL is aimed at reproducibility, so the training script will first print out the running configuration -- the environment variables, versions of various libraries, the full training config, data size, model etc. All the outputs printed on `stdout` are also stored in a `log.txt` file under the output directory specified by `CHECKPOINT.DIR`.\n",
    "\n",
    "You can then see the training stats printed out:\n",
    "```bash\n",
    "INFO 2020-07-01 14:06:34,485 log_hooks.py: 126: Rank: 0; [ep: 0] iter: 1; lr: 0.025; loss: 7.08849; btime(ms): 10386\n",
    "INFO 2020-07-01 14:06:36,878 log_hooks.py: 126: Rank: 0; [ep: 0] iter: 5; lr: 0.025; loss: 8.26936; btime(ms): 2500\n",
    "INFO 2020-07-01 14:06:39,867 log_hooks.py: 126: Rank: 0; [ep: 0] iter: 10; lr: 0.025; loss: 8.94679; btime(ms): 1549\n",
    "INFO 2020-07-01 14:06:42,854 log_hooks.py: 126: Rank: 0; [ep: 0] iter: 15; lr: 0.025; loss: 7.74198; btime(ms): 1231\n",
    "INFO 2020-07-01 14:06:45,857 log_hooks.py: 126: Rank: 0; [ep: 0] iter: 20; lr: 0.025; loss: 7.50769; btime(ms): 1074\n",
    "```\n",
    "\n",
    "In addition, VISSL also prints out the GPU memory usage and the ETA (approximate time for the experiment to finish)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the training config\n",
    "\n",
    "We can now try to understand the train config file.\n",
    "\n",
    "### Data\n",
    "The input data and labels needed to train the model are specified under the \n",
    "`DATA` key. The training and testing data are specified under `DATA.TRAIN` and `DATA.TEST`. \n",
    "For example,\n",
    "```yaml\n",
    "DATA:\n",
    "    TRAIN:\n",
    "      DATA_SOURCES: [disk_folder]\n",
    "      DATA_PATHS: [\"<path to train folder>\"]\n",
    "      LABEL_SOURCES: [disk_folder]\n",
    "      DATASET_NAMES: [imagenet1k]\n",
    "      BATCHSIZE_PER_REPLICA: 64\n",
    "```\n",
    "This specifies that the model will train on the images provided in the folder `DATA.TRAIN.DATA_PATHS` and infer the labels from the directory structure of the images. The model is trained with a batchsize of 64 images/GPU.\n",
    "\n",
    "The image transforms are specified in `TRANSFORMS` and are generally wrap the `torchvision` image transforms. One can easily compose together multiple transforms by specifying them in the config file, or implement their own custom image transforms. VISSL uses such compositionality of data transforms for implementing many self-supervised methods as well.\n",
    "\n",
    "### Model\n",
    "VISSL specifies the model as a `TRUNK` (the base ConvNet) and a `HEAD` (the classification or task-specific parameters). This allows one to cleanly separate the logic between the task itself and the ConvNet. Multiple model trunks (see listing under `vissl/model/trunks`) can be used for the same task.\n",
    "\n",
    "A ResNet-50 model that outputs classification scores for 1000 classes (the number of classes in ImageNet) is specified as\n",
    "```yaml\n",
    "MODEL:\n",
    "    TRUNK:\n",
    "      NAME: resnet\n",
    "      TRUNK_PARAMS:\n",
    "        RESNETS:\n",
    "          DEPTH: 50\n",
    "    HEAD:\n",
    "      PARAMS: [\n",
    "        [\"mlp\", {\"dims\": [2048, 1000]}],\n",
    "      ]\n",
    "```\n",
    "Here `TRUNK` specifies the base ConvNet architecture, and `HEAD` specifies a single fully connected layer (special case of a MLP) that produces 1000 outputs.\n",
    "\n",
    "VISSL automatically sets the model to `eval` mode when using the data in `DATA.TEST`. This ensures that layers such as BatchNorm, Dropout behave correctly when used to report test set accuracies.\n",
    "\n",
    "### Criterion and Optimizer\n",
    "The criterion and optimizer are specified under the `CRITERION` and `OPTIMIZER` keys. VISSL Criterions behave similar to the default `torch.nn` criterions.\n",
    "\n",
    "The `OPTIMIZER` contains information about the base optimizer (SGD in this case) and the learning rate scheduler (`OPTIMIZER.param_schedulers`).\n",
    "\n",
    "### Measuring Accuracy\n",
    "Accuracy meters are specified under `METERS` and measure the top-1 and top-5 accuracies.\n",
    "\n",
    "### Number of GPUs\n",
    "The number of GPUs and number of nodes are specified under `DISTRIBUTED`. VISSL seamlessly runs the **same** code on either a single GPU or across multiple nodes/GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the config\n",
    "\n",
    "### Distributed Training\n",
    "VISSL can easily support distributed training by changing the nodes/GPUs specified under `DISTRIBUTED` and does not require writing any extra code. The *same* code can be run in single GPU or multi-GPU settings.\n",
    "\n",
    "\n",
    "```yaml\n",
    "DISTRIBUTED:\n",
    "    NUM_NODES: 1 # number of nodes\n",
    "    NUM_PROC_PER_NODE: 1 # number of gpus per node\n",
    "```\n",
    "\n",
    "After changing the number of nodes/gpus, you can run your experiment with the same command as earlier \n",
    "```bash\n",
    "python tools/distributed_train.py config=<config> \n",
    "```\n",
    "\n",
    "If running on more than one node, you will need to run this command on each of the nodes. Please note the following:\n",
    "1. The batch size specified in the configs under `DATA.TRAIN.BATCHSIZE_PER_REPLICA` (denoted as *B*) is **per GPU**. So if you run your code on *N* nodes with *G* gpus each, then the total effective batch size is *B*`*`*N*`*`*G*. \n",
    "2. Since running on multiple GPUs changes the effective batch size, you may also want to change the learning rate or use learning rate warmup (see [the ImageNet in 1 hour paper](https://arxiv.org/abs/1706.02677)).\n",
    "\n",
    "\n",
    "### Mixed Precision or FP16 training\n",
    "If you installed VISSL with Apex, you can easily train the model using mixed precision. This requires adding the following lines to the config file under the `MODEL`\n",
    "\n",
    "```yaml\n",
    " AMP_PARAMS:\n",
    "      USE_AMP: True\n",
    "      AMP_ARGS: {\"opt_level\": \"O1\"}\n",
    "```\n",
    "This will run the model using the `O1` setting in apex which should generally result in stable training while saving GPU memory (and possibly faster training depending on the GPU architecture). See the [apex documentation](https://nvidia.github.io/apex/amp.html#opt-levels-and-properties) for more information on what the different mixed precision flags.\n",
    "\n",
    "### Using SyncBatchNorm in the model\n",
    "This can be specified in the config under the `MODEL`\n",
    "\n",
    "```yaml\n",
    "SYNC_BN_CONFIG:\n",
    "      CONVERT_BN_TO_SYNC_BN: True\n",
    "      SYNC_BN_TYPE: pytorch\n",
    "```\n",
    "\n",
    "If you have `apex` installed, you can use a faster version of SyncBatchNorm by\n",
    "\n",
    "```yaml\n",
    "SYNC_BN_CONFIG:\n",
    "      CONVERT_BN_TO_SYNC_BN: True\n",
    "      SYNC_BN_TYPE: apex\n",
    "      GROUP_SIZE: 8 # set to number of GPUs per node for fast performance.\n",
    "```\n",
    "\n",
    "Our model definitions are written such that one can easily replace BatchNorm with other normalization functions (LayerNorm, GroupNorm etc.) by changing arguments in the config file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "bento_kernel_default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
