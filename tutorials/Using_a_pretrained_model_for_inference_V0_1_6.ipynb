{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Using a pretrained model for inference V0.1.6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/facebookresearch/vissl/blob/v0.1.6/tutorials/Using_a_pretrained_model_for_inference_V0_1_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "\n",
        "# This source code is licensed under the MIT license found in the\n",
        "# LICENSE file in the root directory of this source tree."
      ],
      "outputs": [],
      "metadata": {
        "id": "1ndZ6XwI7MYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a pre-trained model in inference mode\n",
        "\n",
        "In this tutorial, we will show how to extract features in inference mode from a VISSL pre-trained trunk.\n",
        "\n",
        "We will concentrate on loading and extracting features from a SimCLR model. \n",
        "This tutorial, however, is portable to any another pre-training methods (MoCo, SimSiam, SwAV, etc). See [here](https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md) for a list of the models in our model zoo. \n",
        "\n",
        "Through it, we will show:\n",
        "\n",
        "1. How to instantiate a model with a pre-training configuration\n",
        "2. Load the weights of the pre-trained model from our model zoo.\n",
        "3. Use it to extract the TRUNK features associated with the VISSL Logo\n",
        "\n",
        "**NOTE:** For a tutorial focused on how to use VISSL to schedule a feature extraction job, please refer to [the dedicated tutorial](https://colab.research.google.com/github/facebookresearch/vissl/blob/stable/tutorials/Feature_Extraction.ipynb).\n",
        "\n",
        "**NOTE:** Please ensure your Collab Notebook has GPU available: `Edit -> Notebook Settings -> select GPU`.\n",
        "\n",
        "**NOTE:** You can make a copy of this tutorial by `File -> Open in playground mode` and make changes there. Please do NOT request access to this tutorial.\n"
      ],
      "metadata": {
        "id": "6XzxTZfKwFNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install VISSL\n",
        "\n",
        "We will start this tutorial by installing VISSL, following the instructions [here](https://github.com/facebookresearch/vissl/blob/master/INSTALL.md#install-vissl-pip-package)."
      ],
      "metadata": {
        "id": "VohdWhBSw69e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Install pytorch version 1.8\n",
        "!pip install torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# install Apex by checking system settings: cuda version, pytorch version, and python version\n",
        "import sys\n",
        "import torch\n",
        "version_str=\"\".join([\n",
        "    f\"py3{sys.version_info.minor}_cu\",\n",
        "    torch.version.cuda.replace(\".\",\"\"),\n",
        "    f\"_pyt{torch.__version__[0:5:2]}\"\n",
        "])\n",
        "print(version_str)\n",
        "\n",
        "# install apex (pre-compiled with optimizer C++ extensions and CUDA kernels)\n",
        "!pip install apex -f https://dl.fbaipublicfiles.com/vissl/packaging/apexwheels/{version_str}/download.html\n",
        "\n",
        "# # clone vissl repository and checkout latest version.\n",
        "!git clone --recursive https://github.com/facebookresearch/vissl.git\n",
        "\n",
        "%cd vissl/\n",
        "\n",
        "!git checkout v0.1.6\n",
        "!git checkout -b v0.1.6\n",
        "\n",
        "# install vissl dependencies\n",
        "!pip install --progress-bar off -r requirements.txt\n",
        "!pip install opencv-python\n",
        "\n",
        "# update classy vision install to current master\n",
        "!pip uninstall -y classy_vision\n",
        "\n",
        "# TODO: fix classy commit here.\n",
        "!pip install classy-vision@https://github.com/facebookresearch/ClassyVision/tarball/main\n",
        "\n",
        "# install vissl dev mode (e stands for editable)\n",
        "!pip install -e .[dev]"
      ],
      "outputs": [],
      "metadata": {
        "id": "R5ISg59KTOqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VISSL should be successfuly installed by now and all the dependencies should be available."
      ],
      "metadata": {
        "id": "u6Fxe3MWxqsI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import vissl\n",
        "import tensorboard\n",
        "import apex\n",
        "import torch"
      ],
      "outputs": [],
      "metadata": {
        "id": "Np6atgoOTPrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a VISSL SimCLR pre-trained model\n"
      ],
      "metadata": {
        "id": "AFEHZ4KdxzWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Download the ResNet-50 Simclr weights from the Model Zoo"
      ],
      "metadata": {
        "id": "x-yyrVpHMehT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!wget -q -O /content/resnet_simclr.torch https://dl.fbaipublicfiles.com/vissl/model_zoo/simclr_rn101_1000ep_simclr_8node_resnet_16_07_20.35063cea/model_final_checkpoint_phase999.torch"
      ],
      "outputs": [],
      "metadata": {
        "id": "mc6wLsB-Ml6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the model associated to the configuration\n",
        "\n",
        "Load the configuration and merge it with the default configuration."
      ],
      "metadata": {
        "id": "n9v8oKuLNCNV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from omegaconf import OmegaConf\n",
        "from vissl.utils.hydra_config import AttrDict\n",
        "\n",
        "from vissl.utils.hydra_config import compose_hydra_configuration, convert_to_attrdict\n",
        "\n",
        "# Config is located at vissl/configs/config/pretrain/simclr/simclr_8node_resnet.yaml.\n",
        "# All other options override the simclr_8node_resnet.yaml config.\n",
        "\n",
        "cfg = [\n",
        "  'config=pretrain/simclr/simclr_8node_resnet.yaml',\n",
        "  'config.MODEL.WEIGHTS_INIT.PARAMS_FILE=/content/resnet_simclr.torch', # Specify path for the model weights.\n",
        "  'config.MODEL.FEATURE_EVAL_SETTINGS.EVAL_MODE_ON=True', # Turn on model evaluation mode.\n",
        "  'config.MODEL.FEATURE_EVAL_SETTINGS.FREEZE_TRUNK_ONLY=True', # Freeze trunk. \n",
        "  'config.MODEL.FEATURE_EVAL_SETTINGS.EXTRACT_TRUNK_FEATURES_ONLY=True', # Extract the trunk features, as opposed to the HEAD.\n",
        "  'config.MODEL.FEATURE_EVAL_SETTINGS.SHOULD_FLATTEN_FEATS=False', # Do not flatten features.\n",
        "  'config.MODEL.FEATURE_EVAL_SETTINGS.LINEAR_EVAL_FEAT_POOL_OPS_MAP=[[\"res5avg\", [\"Identity\", []]]]' # Extract only the res5avg features.\n",
        "]\n",
        "\n",
        "# Compose the hydra configuration.\n",
        "cfg = compose_hydra_configuration(cfg)\n",
        "# Convert to AttrDict. This method will also infer certain config options\n",
        "# and validate the config is valid.\n",
        "_, cfg = convert_to_attrdict(cfg)"
      ],
      "outputs": [],
      "metadata": {
        "id": "NCwpxr5lNBQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then build the model:"
      ],
      "metadata": {
        "id": "A8MvkUNePfiP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from vissl.models import build_model\n",
        "\n",
        "model = build_model(cfg.MODEL, cfg.OPTIMIZER)"
      ],
      "outputs": [],
      "metadata": {
        "id": "IJPhQB5kPdIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the pre-trained weights"
      ],
      "metadata": {
        "id": "ayNfZrhdPnOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from classy_vision.generic.util import load_checkpoint\n",
        "from vissl.utils.checkpoint import init_model_from_consolidated_weights\n",
        "\n",
        "# Load the checkpoint weights.\n",
        "weights = load_checkpoint(checkpoint_path=cfg.MODEL.WEIGHTS_INIT.PARAMS_FILE)\n",
        "\n",
        "\n",
        "# Initializei the model with the simclr model weights.\n",
        "init_model_from_consolidated_weights(\n",
        "    config=cfg,\n",
        "    model=model,\n",
        "    state_dict=weights,\n",
        "    state_dict_key_name=\"classy_state_dict\",\n",
        "    skip_layers=[],  # Use this if you do not want to load all layers\n",
        ")\n",
        "\n",
        "print(\"Weights have loaded\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights have loaded\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-xbwy1uPsT6",
        "outputId": "7229d292-f8c4-403e-f98e-1a58a931cd16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting the Trunk Output on the VISSL Logo."
      ],
      "metadata": {
        "id": "irMB0q4ZP2nw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!wget -q -O /content/test_image.jpg https://raw.githubusercontent.com/facebookresearch/vissl/master/.github/logo/Logo_Color_Light_BG.png"
      ],
      "outputs": [],
      "metadata": {
        "id": "rW0TIFz7QN2b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def extract_features(path):\n",
        "  image = Image.open(path)\n",
        "\n",
        "  # Convert images to RGB. This is important\n",
        "  # as the model was trained on RGB images.\n",
        "  image = image.convert(\"RGB\")\n",
        "\n",
        "  # Image transformation pipeline.\n",
        "  pipeline = transforms.Compose([\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "  ])\n",
        "  x = pipeline(image)\n",
        "\n",
        "  features = model(x.unsqueeze(0))\n",
        "\n",
        "  features_shape = features[0].shape\n",
        "\n",
        "  print(f\"Features extracted have the shape: { features_shape }\")\n",
        "\n",
        "extract_features(\"/content/test_image.jpg\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted have the shape: torch.Size([1, 2048, 1, 1])\n"
          ]
        }
      ],
      "metadata": {
        "id": "hDpzK3TOP6Rw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa26dcfc-35ac-4818-9afe-355a8d7f9f52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a list with as many representation layers as specified in the configuration. Specifically `cfg.config.MODEL.FEATURE_EVAL_SETTINGS.LINEAR_EVAL_FEAT_POOL_OPS_MAP` asks for one representation layer, namely the res5avg layer, so we only have one output."
      ],
      "metadata": {
        "id": "m-8uspwrQ8a-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Features from Model Head"
      ],
      "metadata": {
        "id": "nEg2WteJ64CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us see how to extract the model's head features. Let's use the supervised RN-50 VISSL model from the [model zoo](https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md) as an example. These settings should extend to many use cases -- for example if you have fine-tuned a simclr model on imagenet and wish to extract the model output logits.\n",
        "\n",
        "For running jobs to extract all the features from a vissl dataset, please see the [feature extraction tutorial](TODO THIS LINK)."
      ],
      "metadata": {
        "id": "1DvYGxQJ6Hm8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!wget -q -O /content/resnet_in1k.torch https://dl.fbaipublicfiles.com/vissl/model_zoo/sup_rn50_in1k_ep105_supervised_8gpu_resnet_17_07_20.733dbdee/model_final_checkpoint_phase208.torch"
      ],
      "outputs": [],
      "metadata": {
        "id": "7xvs5lFd6GOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from omegaconf import OmegaConf\n",
        "from vissl.utils.hydra_config import AttrDict\n",
        "\n",
        "from vissl.utils.hydra_config import compose_hydra_configuration, convert_to_attrdict\n",
        "\n",
        "# Config is located at vissl/configs/config/pretrain/simclr/simclr_8node_resnet.yaml.\n",
        "# All other options override the simclr_8node_resnet.yaml config.\n",
        "\n",
        "# Note here we freeze the trunk and the head, and specify that we want to eval\n",
        "# with the trunk and head.\n",
        "cfg = [\n",
        "  'config=pretrain/supervised/supervised_1gpu_resnet_example.yaml',\n",
        "  'config.MODEL.WEIGHTS_INIT.PARAMS_FILE=/content/resnet_in1k.torch', # Specify path for the model weights.\n",
        "  'config.MODEL.FEATURE_EVAL_SETTINGS.EVAL_MODE_ON=True', # Turn on model evaluation mode.\n",
        "  'config.MODEL.FEATURE_EVAL_SETTINGS.FREEZE_TRUNK_AND_HEAD=True', # Freeze trunk. \n",
        "  'config.MODEL.FEATURE_EVAL_SETTINGS.EVAL_TRUNK_AND_HEAD=True', # Extract the trunk features, as opposed to the HEAD.\n",
        "]\n",
        "\n",
        "# NOTE: After this everything is the same as the above example of extracting \n",
        "# the TRUNK features.\n",
        "\n",
        "# Compose the hydra configuration.\n",
        "cfg = compose_hydra_configuration(cfg)\n",
        "\n",
        "# Convert to AttrDict. This method will also infer certain config options\n",
        "# and validate the config is valid.\n",
        "_, cfg = convert_to_attrdict(cfg)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Fg9eOnmp7FB9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Build the model\n",
        "from vissl.models import build_model\n",
        "from vissl.utils.checkpoint import init_model_from_consolidated_weights\n",
        "\n",
        "model = build_model(cfg.MODEL, cfg.OPTIMIZER)\n",
        "\n",
        "# Load the checkpoint weights.\n",
        "weights = load_checkpoint(checkpoint_path=cfg.MODEL.WEIGHTS_INIT.PARAMS_FILE)\n",
        "\n",
        "# Initializei the model with the simclr model weights.\n",
        "init_model_from_consolidated_weights(\n",
        "    config=cfg,\n",
        "    model=model,\n",
        "    state_dict=weights,\n",
        "    state_dict_key_name=\"classy_state_dict\",\n",
        "    skip_layers=[],  # Use this if you do not want to load all layers\n",
        ")\n",
        "\n",
        "print(\"Weights have loaded\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights have loaded\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0Mu53gz7q23",
        "outputId": "f023216c-a06d-40aa-efa6-1327303ffbe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see below, our model has an output of 1000 features for each of the 1000 imagenet classes."
      ],
      "metadata": {
        "id": "NWklYMpn9JUA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "extract_features(\"/content/test_image.jpg\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted have the shape: torch.Size([1, 1000])\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_QXkKqh8Xhm",
        "outputId": "29877a4c-3add-4599-ef11-316cd759e237"
      }
    }
  ]
}