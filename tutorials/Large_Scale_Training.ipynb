{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Large Scale Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XzxTZfKwFNo"
      },
      "source": [
        "# Large Scale Training with VISSL Training (mixed precision, LARC, ZeRO etc)\n",
        "\n",
        "In this tutorial, show configuration settings that users can set for training large models.\n",
        "\n",
        "You can make a copy of this tutorial by `File -> Open in playground mode` and make changes there. DO NOT request access to this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFEHZ4KdxzWq"
      },
      "source": [
        "# Using LARC\n",
        "\n",
        "LARC (Large Batch Training of Convolutional Networks) is a technique proposed by **Yang You, Igor Gitman, Boris Ginsburg** in https://arxiv.org/abs/1708.03888 for improving the convergence of large batch size trainings.\n",
        "LARC uses the ratio between gradient and parameter magnitudes is used to calculate an adaptive local learning rate for each individual parameter.\n",
        "\n",
        "See the [LARC paper](<https://arxiv.org/abs/1708.03888>) for calculation of learning rate. In practice, it modifies the gradients of parameters as a proxy\n",
        "for modifying the learning rate of the parameters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKfAQdqzBPC9"
      },
      "source": [
        "## How to enable LARC\n",
        "\n",
        "VISSL supports the LARC implementation from [NVIDIA's Apex LARC](https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py). To use LARC, users need to set config option\n",
        ":code:`OPTIMIZER.use_larc=True`. VISSL exposes LARC parameters that users can tune. Full list of LARC parameters exposed by VISSL:\n",
        "\n",
        "\n",
        "```yaml\n",
        "OPTIMIZER:\n",
        "  name: \"sgd\"\n",
        "  use_larc: False  # supported for SGD only for now\n",
        "  larc_config:\n",
        "    clip: False\n",
        "    eps: 1e-08\n",
        "    trust_coefficient: 0.001\n",
        "```\n",
        "\n",
        "**NOTE:** LARC is currently supported for SGD optimizer only in VISSL.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xtRB6l6BQ9x"
      },
      "source": [
        "# Using Apex\n",
        "\n",
        "\n",
        "In order to use Apex, VISSL provides `anaconda` and `pip` packages of Apex (compiled with Optimzed C++ extensions/CUDA kernels). The Apex\n",
        "packages are provided for all versions of `CUDA (9.2, 10.0, 10.1, 10.2, 11.0), PyTorch >= 1.4 and Python >=3.6 and <=3.9`.\n",
        "\n",
        "Follow VISSL's instructions to [install apex in pip](https://github.com/facebookresearch/vissl/blob/master/INSTALL.md#step-2-install-pytorch-opencv-and-apex-pip) and instructions to [install apex in conda](https://github.com/facebookresearch/vissl/blob/master/INSTALL.md#step-3-install-apex-conda>)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywEsw-PC_okg"
      },
      "source": [
        "# Using Mixed Precision\n",
        "\n",
        "Many self-supervised approaches leverage mixed precision training by default for better training speed and reducing the model memory requirement.\n",
        "For this, we use [NVIDIA Apex Library with AMP](https://nvidia.github.io/apex/amp.html#o1-mixed-precision-recommended-for-typical-use).\n",
        "\n",
        "Users can tune the AMP level to the levels supported by NVIDIA. See [this for details on Apex amp levels](https://nvidia.github.io/apex/amp.html#opt-levels).\n",
        "\n",
        "To use Mixed precision training, one needs to set the following parameters in configuration file:\n",
        "\n",
        "\n",
        "```yaml\n",
        "MODEL:\n",
        "  AMP_PARAMS:\n",
        "    USE_AMP: True\n",
        "    # Use O1 as it is robust and stable than O3. If you want to use O3, we recommend\n",
        "    # the following setting:\n",
        "    # {\"opt_level\": \"O3\", \"keep_batchnorm_fp32\": True, \"master_weights\": True, \"loss_scale\": \"dynamic\"}\n",
        "    AMP_ARGS: {\"opt_level\": \"O1\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W5emDVL_sDP"
      },
      "source": [
        "# Using ZeRO\n",
        "\n",
        "**ZeRO: Memory Optimizations Toward Training Trillion Parameter Models** is a technique developed by **Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He** in [this paper](https://arxiv.org/abs/1910.02054).\n",
        "When training models with billions of parameters, GPU memory becomes a bottleneck. ZeRO can offer 4x to 8x memory reductions in memory thus allowing to fit larger models in memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHf2PJxeB6Ho"
      },
      "source": [
        "## How ZeRO works?\n",
        "\n",
        "\n",
        "Memory requirement of a model can be broken down roughly into:\n",
        "\n",
        "1. activations memory\n",
        "2. model parameters\n",
        "3. parameters momentum buffers (optimizer state)\n",
        "4. parameters gradients\n",
        "\n",
        "ZeRO *shards* the optimizer state and the parameter gradients onto different devices and reduces the memory needed per device.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4SzEZ4aB8IK"
      },
      "source": [
        "## How to use ZeRO in VISSL?\n",
        "\n",
        "VISSL uses [FAIRScale](https://github.com/facebookresearch/fairscale)_ library which implements ZeRO in PyTorch.\n",
        "Using VISSL in ZeRO involves no code changes and can simply be done by setting some configuration options in the yaml files.\n",
        "\n",
        "In order to use ZeRO, user needs to set `OPTIMIZER.name=zero` and nest the desired optimizer (for example SGD) settings in `OPTIMIZER.base_optimizer`.\n",
        "\n",
        "An example for using ZeRO with LARC and SGD optimization:\n",
        "```yaml\n",
        "OPTIMIZER:\n",
        "  name: zero\n",
        "  base_optimizer:\n",
        "    name: sgd\n",
        "    use_larc: False\n",
        "    larc_config:\n",
        "      clip: False\n",
        "      trust_coefficient: 0.001\n",
        "      eps: 0.00000001\n",
        "    weight_decay: 0.000001\n",
        "    momentum: 0.9\n",
        "    nesterov: False\n",
        "```\n",
        "\n",
        "**NOTE**: ZeRO works seamlessly with LARC and mixed precision training. Using ZeRO with activation checkpointing is not yet enabled primarily due to manual gradient reduction need for activation checkpointing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kXUmFEv_vJ7"
      },
      "source": [
        "# Using Stateful Data Sampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJewF2T6CBDJ"
      },
      "source": [
        "## Issue with PyTorch DataSampler for large data training\n",
        "\n",
        "\n",
        "PyTorch default [torch.utils.data.distributed.DistributedSampler](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py#L12) is the default sampler used for many trainings. However, it becomes limiting to use this sampler in case of large batch size trainings for 2 reasons:\n",
        "\n",
        "- Using PyTorch `DataSampler`, each trainer shuffles the full data (assuming shuffling is used) and then each trainer gets a view of this shuffled data. If the dataset is large (100 millions, 1 billion or more), generating very large permutation\n",
        "on each trainer can lead to large CPU memory consumption per machine. Hence, it becomes difficult to use the PyTorch default `DataSampler` when user wants to train on large data and for several epochs (for example: 10 epochs of 100M images).\n",
        "\n",
        "- When using PyTorch `DataSampler` and the training is resumed, the sampler will serve the full dataset. However, in case of large data trainings (like 1 billion images or more), one mostly trains for 1 epoch only.\n",
        "  In such cases, when the training resumes from the middle of the epoch, the sampler will serve the full 1 billion images which is not what we want.\n",
        "\n",
        "\n",
        "To solve both the above issues, VISSL provides a custom samplier `StatefulDistributedSampler` which inherits from the PyTorch `DistributedSampler` and fixes the above issues in following manner:\n",
        "\n",
        "- Sampler creates the view of the data per trainer and then shuffles only the data that trainer is supposed to view. This keeps the CPU memory requirement expected.\n",
        "\n",
        "- Sampler adds a member `start_iter` which tracks what iteration number of the given epoch model is at. When the training is used, the `start_iter` will be properly set to the last iteration number and the sampler will serve only the remainder of data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07OEhtl8CToA"
      },
      "source": [
        "## How to use VISSL custom DataSampler\n",
        "\n",
        "\n",
        "Using VISSL provided custom samplier `StatefulDistributedSampler` is extremely easy and involves simply setting the correct configuration options as below:\n",
        "\n",
        "\n",
        "```yaml\n",
        "DATA:\n",
        "  TRAIN:\n",
        "    USE_STATEFUL_DISTRIBUTED_SAMPLER: True\n",
        "  TEST:\n",
        "    USE_STATEFUL_DISTRIBUTED_SAMPLER: True\n",
        "```\n",
        "\n",
        "**NOTE**: Users can use `StatefulDistributedSampler` for only training dataset and use PyTorch default :code:`DataSampler` if desired i.e. it is not mandatory to use the same sampler type for all data splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE6e3AEP_1Az"
      },
      "source": [
        "# Activation Checkpointing\n",
        "\n",
        "Activation checkpointing is a very powerful technique to reduce the memory requirement of a model. This is especially useful when training very large models with billions of parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCl20zHBCj_Z"
      },
      "source": [
        "## How it works?\n",
        "\n",
        "Activation checkpointing trades compute for memory. It discards intermediate activations during the forward pass, and recomputes them during the backward pass. In\n",
        "our experiments, using activation checkpointing, we observe negligible compute overhead in memory-bound settings while getting big memory savings.\n",
        "\n",
        "In summary, This technique offers 2 benefits:\n",
        "\n",
        "- saves gpu memory that can be used to fit large models\n",
        "- allows increasing training batch size for a given model\n",
        "\n",
        "We recommend users to read the documentation available [here](https://pytorch.org/docs/stable/checkpoint.html) for further details on activation checkpointing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bp0UZTGCle5"
      },
      "source": [
        "## How to use activation checkpointing in VISSL?\n",
        "\n",
        "VISSL integrates activation checkpointing implementation directly from PyTorch available [here](https://pytorch.org/docs/stable/checkpoint.html).\n",
        "Using activation checkpointing in VISSL is extremely easy and doable with simple settings in the configuration file. The settings required are as below:\n",
        "\n",
        "```yaml\n",
        "MODEL:\n",
        "  ACTIVATION_CHECKPOINTING:\n",
        "    # whether to use activation checkpointing or not\n",
        "    USE_ACTIVATION_CHECKPOINTING: True\n",
        "    # how many times the model should be checkpointed. User should tune this parameter\n",
        "    # and find the number that offers best memory saving and compute tradeoff.\n",
        "    NUM_ACTIVATION_CHECKPOINTING_SPLITS: 8\n",
        "DISTRIBUTED:\n",
        "  # if True, does the gradient reduction in DDP manually. This is useful during the\n",
        "  # activation checkpointing and sometimes saving the memory from the pytorch gradient\n",
        "  # buckets.\n",
        "  MANUAL_GRADIENT_REDUCTION: True\n",
        "```"
      ]
    }
  ]
}
