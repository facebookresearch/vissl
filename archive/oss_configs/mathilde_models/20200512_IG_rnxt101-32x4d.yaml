VERBOSE: False
LOG_FREQUENCY: 10
TEST_ONLY: False
TEST_MODEL: False
SEED_VALUE: 0
MULTI_PROCESSING_METHOD: forkserver
MONITOR_PERF_STATS: False
DATA:
  TRAIN:
    DATA_SOURCES: [everstore]
    DATASET_NAMES: [instagram_1billion]
    # DATASET_NAMES: [instagram_100M]
    BATCHSIZE_PER_REPLICA: 64
    LABEL_TYPE: sample_index    # just an implementation detail. Label isn't used
    TRANSFORMS:
      - name: ImgPilToMultiResCrops
        nmb_duplicates: 6
        size_crops: [224, 96, 96, 96, 96]
        type_crops: [0, 1, 1, 1, 1]
        nmb_crops: [2, 1, 1, 1, 1]
        crop_scales: [[0.08, 1], [0.6, 1], [0.6, 1], [0.6, 1], [0.6, 1]]
        crop_size_typePIRL: 255
      - name: RandomHorizontalFlip
        p: 0.5
      - name: ImgPilColorDistortion
        strength: 1.0
      - name: ImgPilGaussianBlur
        kernel: 23
        p: 0.5
        radius_min: 0.1
        radius_max: 2.0
      - name: ImgToTensor
      - name: Normalize
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]
    COLLATE_FUNCTION: multires_collator
    USE_STATEFUL_DISTRIBUTED_SAMPLER: True
    MMAP_MODE: True
    COPY_TO_LOCAL_DISK: False
    COPY_DESTINATION_DIR: /tmp/instagram_1billion/
    # COPY_DESTINATION_DIR: /tmp/instagram_100M/
    DROP_LAST: True
TRAINER:
  TRAIN_STEP_NAME: standard
METERS: {}
MODEL:
  TRUNK:
    NAME: resnext101-32x4d
  HEAD:
    PARAMS: [
    ["oto_head", {"dims": [2048, 2048, 128], "use_bn": True, "nmb_clusters": [3000]}],
    ]
  TEMP_FROZEN_PARAMS_ITER_MAP: [
    ['module.heads.0.prototypes0.weight', 313],
  ]
  SYNC_BN_CONFIG:
    CONVERT_BN_TO_SYNC_BN: True
    # SYNC_BN_TYPE: pytorch
    SYNC_BN_TYPE: apex
  AMP_PARAMS:
    USE_AMP: True
    AMP_ARGS: {"opt_level": "O1"}
CRITERION:
    name: oto_loss
    MUL_TARGET_DATA_VALID: True
    SINKHORN_KNOPP_LOSS:
      NORMALIZE_LAST_LAYER: True
      USE_DOUBLE_PRECISION: False
      FILTER_INVALID_IMAGES: False
      TEMPERATURE: 0.1
      LAMBDA: 10
      MAX_ITERS: 3
      DUPLICATES_FOR_ASSIGN: [0, 1]
      EPSILON: 0.000001
OPTIMIZER:
    name: sgd
    use_larc: True
    larc_config:
      clip: False
      trust_coefficient: 0.001
      eps: 0.00000001
    weight_decay: 0.000001
    momentum: 0.9
    nesterov: False
    num_epochs: 1
    # num_epochs: 2
    # num_epochs: 10
    # num_epochs: 100       # for IG-100M
    regularize_bn: True
    regularize_bias: True
    param_schedulers:
      lr:
        base_value: 4.8
        name: composite
        schedulers:
          - name: linear
            start_value: 0.3
            # start_value: 0.0
            end_value: 4.8
          - name: cosine
            start_value: 4.8
            end_value: 0.0000
            # end_value: 0.0048
            # end_value: 0.00048
        # update_interval: epoch
        update_interval: step
        interval_scaling: [rescaled, fixed]
        # lengths: [0.1, 0.9]
        lengths: [0.0125, 0.9875]       # 800ep IN1K = 1ep IG-1B
        # lengths: [0.00625, 0.99375]     # 1600ep IN1K = 2ep IG-1B
        # lengths: [0.0012821, 0.9987179]   # 7800ep IN1K = 10ep IG-1B = 100 ep IG-100M
DISTRIBUTED:
  DISTR_ON: True
  BACKEND: nccl
  NUM_NODES: 8
  NUM_PROC_PER_NODE: 8
  # RUN_ID: tui7ui6fyvcioy8fv6yiy
  RUN_ID: turdcxkutid5utfvco8yvoi
  INIT_METHOD: zeus
  NCCL_DEBUG: True
MACHINE:
  NUM_DATALOADER_WORKERS: 8
  DEVICE: gpu
CHECKPOINT:
  DIR: /mnt/vol/gfsai-bistro2-east/ai-group/users/prigoyal/distributed/
  AUTO_RESUME: True
  CHECKPOINT_FREQUENCY: 1           # every phase since data is big
  CHECKPOINT_ITER_FREQUENCY: 310    # equals 1epoch of imagenet-1k
