VERBOSE: False
LOG_FREQUENCY: 10
TEST_ONLY: False
TEST_MODEL: False
SEED_VALUE: 0
MULTI_PROCESSING_METHOD: forkserver
MONITOR_PERF_STATS: True
DATA:
  TRAIN:
    DATA_SOURCES: [disk]
    DATASET_NAMES: [imagenet1k]
    DATA_PATHS: ['/datasets01_101/imagenet_full_size/061417/train']
    BATCHSIZE_PER_REPLICA: 64
    LABEL_TYPE: sample_index    # just an implementation detail. Label isn't used
    TRANSFORMS:
      - name: ImgReplicatePil
        num_times: 2
      - name: RandomResizedCrop
        size: 224
      - name: RandomHorizontalFlip
        p: 0.5
      - name: ImgPilColorDistortion
        strength: 1.0
      - name: ImgPilGaussianBlur
        kernel: 23
        p: 0.5
        radius_min: 0.1
        radius_max: 2.0
      - name: ToTensor
      - name: Normalize
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]
    COLLATE_FUNCTION: flatten_collator
    MMAP_MODE: True
    COPY_TO_LOCAL_DISK: True
    COPY_DESTINATION_DIR: /scratch/slurm_tmpdir/
    DROP_LAST: True
TRAINER:
  TRAIN_STEP_NAME: standard
METERS: {}
MODEL:
  TRUNK:
    # NAME: resnet50_NaiveSyncBN
    NAME: resnet50
    # NAME: resnet101
    # NAME: resnet50_w2
  HEAD:
    PARAMS: [
      ["mlp", {"dims": [2048, 2048], "use_relu": True}],
      ["mlp", {"dims": [2048, 128]}],
      # ["mlp", {"dims": [4096, 4096], "use_relu": True}],
      # ["mlp", {"dims": [4096, 128]}],
    ]
  SYNC_BN_CONFIG:
    CONVERT_BN_TO_SYNC_BN: False
    # SYNC_BN_TYPE: pytorch
    SYNC_BN_TYPE: apex
CRITERION:
    name: info_nce_loss
    MUL_TARGET_DATA_VALID: True
    INFO_NCE_LOSS:
      TEMPERATURE: 0.1
      NUM_POSITIVES: 2
      BUFFER_PARAMS:
        EMBEDDING_DIM: 128
        EFFECTIVE_BATCH_SIZE: 8192
OPTIMIZER:
    name: sgd
    use_larc: True
    larc_config:
      clip: False
      trust_coefficient: 0.001
      eps: 0.00000001
    weight_decay: 0.000001
    momentum: 0.9
    nesterov: False
    num_epochs: 100
    # num_epochs: 200
    # num_epochs: 400
    # num_epochs: 500
    # num_epochs: 600
    # num_epochs: 800
    # num_epochs: 1000
    regularize_bn: False
    regularize_bias: True
    param_schedulers:
      lr:
        base_value: 4.8
        name: composite
        schedulers:
          - name: linear
            start_value: 0.6
            end_value: 4.8
          - name: cosine
            start_value: 4.8
            end_value: 0.0000
        update_interval: step
        interval_scaling: [rescaled, fixed]
        lengths: [0.1, 0.9]               # 100ep
        # lengths: [0.05, 0.95]             # 200ep
        # lengths: [0.025, 0.975]           # 400ep
        # lengths: [0.02, 0.98]             # 500ep
        # lengths: [0.0166667, 0.9833333]   # 600ep
        # lengths: [0.0125, 0.9875]         # 800ep
        # lengths: [0.01, 0.99]         # 1000ep
DISTRIBUTED:
  DISTR_ON: True
  BACKEND: nccl
  NUM_NODES: 8    # gives batch size of 20
  NUM_PROC_PER_NODE: 8
  RUN_ID: t9ui75tf8y6foyv8yf867ftciyu     # 100ep new version of loss p100
  INIT_METHOD: tcp
  NCCL_DEBUG: True
MACHINE:
  NUM_DATALOADER_WORKERS: 8
  DEVICE: gpu
CHECKPOINT:
  DIR: "."
  AUTO_RESUME: True
  CHECKPOINT_FREQUENCY: 5
