# @package _global_
config:
  TRAINER:
    TASK_NAME: self_supervision_fsdp_task
  MODEL:
    FEATURE_EVAL_SETTINGS:
      EVAL_MODE_ON: True
      FREEZE_TRUNK_ONLY: True
      EXTRACT_TRUNK_FEATURES_ONLY: True
      SHOULD_FLATTEN_FEATS: True
    TRUNK:
      NAME: regnet_fsdp
      REGNET:
        block_type: res_bottleneck_block
        depth: 27
        group_width: 1010
        w_0: 1744
        w_a: 620.83
        w_m: 2.52
        stage_checkpoints: [[2], [7], [9, 17], []]
    HEAD:
      PARAMS: []
    SYNC_BN_CONFIG:
      # No sync BN needed in eval mode
      CONVERT_BN_TO_SYNC_BN: False
      SYNC_BN_TYPE: pytorch
    FSDP_CONFIG:
      # Disabling AUTO_SETUP_FSDP so that:
      # - flatten_parameters is not forced to True
      # - construct_single_param_group_only is not forced to True
      AUTO_SETUP_FSDP: False
      AUTO_WRAP_THRESHOLD: 100000000
      flatten_parameters: False
      compute_dtype: float32
      mixed_precision: False
      fp32_reduce_scatter: False
    AMP_PARAMS:
      USE_AMP: False
      AMP_TYPE: pytorch
    CUDA_CACHE:
      CLEAR_CUDA_CACHE: True
      CLEAR_FREQ: 5000
    ACTIVATION_CHECKPOINTING:
      # With feature extraction, we do not need activation checkpointing
      # because the trunk is used in evaluation mode and does not need
      # to accumulate any gradient
      USE_ACTIVATION_CHECKPOINTING: False
    WEIGHTS_INIT:
      PARAMS_FILE: "manifold://ssl_framework/tree/qduval/consolidated_10b/model_iteration120600_sliced.torch"
      STATE_DICT_KEY_NAME: classy_state_dict
      SKIP_LAYERS: ["num_batches_tracked"]
