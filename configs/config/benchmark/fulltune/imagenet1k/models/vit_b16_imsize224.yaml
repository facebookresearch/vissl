# @package _global_
config:
  MODEL:
    TRUNK: # B-16
      NAME: vision_transformer
      VISION_TRANSFORMERS:
        IMAGE_SIZE: 224
        PATCH_SIZE: 16
        HIDDEN_DIM: 768
        NUM_LAYERS: 12
        NUM_HEADS: 12
        MLP_DIM: 3072
        CLASSIFIER: token
        DROPOUT_RATE: 0
        ATTENTION_DROPOUT_RATE: 0
        QKV_BIAS: True
        DROP_PATH_RATE: 0.1 # stochastic depth dropout probability
    HEAD:
      PARAMS: [
        ["mlp", {"dims": [768, 1000]}],
      ]
