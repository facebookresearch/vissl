# @package _global_
config:
  DATA:
    TRAIN:
      BATCHSIZE_PER_REPLICA: 8
    TEST:
      BATCHSIZE_PER_REPLICA: 8
  MODEL:
    FEATURE_EVAL_SETTINGS:
      EVAL_MODE_ON: True
      EVAL_TRUNK_AND_HEAD: True
    TRUNK:
      NAME: regnet
      REGNET:
        depth: 27
        w_0: 640
        w_a: 230.83
        w_m: 2.53
        group_width: 373
    HEAD:
      PARAMS_MULTIPLIER: 1
      PARAMS: [
        ["swav_head", {"normalize_feats": False, "skip_last_bn": False, "use_bn": True, "dims": [10444, 8192], "num_clusters": []}],
        ['mlp', {'dims': [8192, 2]}]
      ]
    SYNC_BN_CONFIG:
      CONVERT_BN_TO_SYNC_BN: False
      SYNC_BN_TYPE: pytorch
  DISTRIBUTED:
    NUM_NODES: 2  # we can't fit the model on 16GB machine so instead use 2 nodes
    INIT_METHOD: tcp
  OPTIMIZER:
    name: sgd
    weight_decay: 1e-6
    momentum: 0.9
    num_epochs: 20
    nesterov: False
    regularize_bn: True
    regularize_bias: False
    param_schedulers:
      lr:
        auto_lr_scaling:
          auto_scale: True
          base_value: 0.01
          base_lr_batch_size: 256
        milestones: [10] # epochs at which to drop the learning rate (N vals)
        values: [0.01, 0.001] # the exact values of learning rate (N+1 vals)
        name: multistep
        update_interval: epoch
