# @package _global_
config:
  DATA:
    TRAIN:
      BATCHSIZE_PER_REPLICA: 8
    TEST:
      BATCHSIZE_PER_REPLICA: 8
  MODEL:
    TRUNK:
      NAME: regnet
      REGNET:
        depth: 27
        w_0: 640
        w_a: 230.83
        w_m: 2.53
        group_width: 373
    HEAD:
      PARAMS: [['mlp', {'dims': [10444, 2]}]]
      PARAMS_MULTIPLIER: 33
    SYNC_BN_CONFIG:
      CONVERT_BN_TO_SYNC_BN: True
      SYNC_BN_TYPE: pytorch
  DISTRIBUTED:
    NUM_NODES: 2  # we can't fit the model on 16GB machine so instead use 2 nodes
    INIT_METHOD: tcp
  # Use different learning rates for head and trunk.
  OPTIMIZER:
    name: sgd
    weight_decay: 1e-6
    momentum: 0.9
    num_epochs: 40
    nesterov: False
    regularize_bn: True
    regularize_bias: True
    head_optimizer_params:
      use_different_lr: True
      use_different_wd: True
      weight_decay: 1e-6
    param_schedulers:
      lr:
        auto_lr_scaling:
          auto_scale: True
          base_value: 0.01
          base_lr_batch_size: 256
        milestones: [20] # epochs at which to drop the learning rate (N vals)
        values: [0.01, 0.001] # the exact values of learning rate (N+1 vals)
        name: multistep
        update_interval: epoch
      lr_head:
        auto_lr_scaling:
          auto_scale: True
          base_value: 0.01
          base_lr_batch_size: 256
          scaling_type: "linear"
        name: "multistep"
        update_interval: "epoch"
        values: [0.01, 0.001]
        milestones: [20]
