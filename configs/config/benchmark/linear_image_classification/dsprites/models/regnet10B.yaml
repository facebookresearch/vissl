# @package _global_
config:
  TRAINER:
    TASK_NAME: self_supervision_fsdp_task
  DATA:
    TRAIN:
      BATCHSIZE_PER_REPLICA: 32
    TEST:
      BATCHSIZE_PER_REPLICA: 32
  OPTIMIZER:
    use_larc: False
    regularize_bn: False
    regularize_bias: True
    construct_single_param_group_only: True
  MODEL:
    FEATURE_EVAL_SETTINGS:
      LINEAR_EVAL_FEAT_POOL_OPS_MAP: [
        # ["res3", ["AdaptiveAvgPool2d", [[2, 2]]]],

        ["res4", ["AdaptiveAvgPool2d", [[2, 1]]]],
        ["res4", ["AdaptiveAvgPool2d", [[1, 2]]]],

        ["res5", ["AdaptiveAvgPool2d", [[2, 1]]]],
        ["res5", ["AdaptiveAvgPool2d", [[1, 2]]]],

        ["avgpool", ["Identity", []]],
        ["avgpool", ["Identity", []]],
      ]
    TRUNK:
      NAME: regnet_fsdp
      REGNET:
        depth: 27
        w_0: 1744
        w_a: 620.83
        w_m: 2.52
        group_width: 1010
        block_type: res_bottleneck_block
        stage_checkpoints: [[2], [7], [9, 17], []]
    HEAD:
      PARAMS: [
        # ["eval_mlp_fsdp", {"in_channels": 4040, "dims": [16160, 16]}],

        ["eval_mlp_fsdp", {"in_channels": 11110, "dims": [22220, 16]}],
        ["eval_mlp_fsdp", {"in_channels": 11110, "dims": [22220, 16]}],

        ["eval_mlp_fsdp", {"in_channels": 28280, "dims": [56560, 16]}],
        ["eval_mlp_fsdp", {"in_channels": 28280, "dims": [56560, 16]}],

        ["eval_mlp_fsdp", {"in_channels": 28280, "dims": [28280, 16]}],
        ["mlp_fsdp", {"dims": [28280, 16]}],
      ]
    SYNC_BN_CONFIG:
      CONVERT_BN_TO_SYNC_BN: True
      SYNC_BN_TYPE: pytorch
      GROUP_SIZE: 0  # global sync
    AMP_PARAMS:
      USE_AMP: False
      AMP_TYPE: pytorch
    FSDP_CONFIG:
      # Disabling AUTO_SETUP_FSDP so that:
      # - flatten_parameters is not forced to True
      # - construct_single_param_group_only is not forced to True
      AUTO_SETUP_FSDP: False
      AUTO_WRAP_THRESHOLD: 100000000
      flatten_parameters: False
      compute_dtype: float16
      mixed_precision: True
      fp32_reduce_scatter: False
    CUDA_CACHE:
      CLEAR_CUDA_CACHE: True
      CLEAR_FREQ: 5000
    ACTIVATION_CHECKPOINTING:
      # With linear evaluation, we do not need activation checkpointing
      # because the trunk is used in evaluation mode and does not need
      # to accumulate any gradient
      USE_ACTIVATION_CHECKPOINTING: False
