# @package _global_
config:
  TRAINER:
    TASK_NAME: self_supervision_fsdp_task
  DATA:
    TRAIN:
      BATCHSIZE_PER_REPLICA: 32
    TEST:
      BATCHSIZE_PER_REPLICA: 32
  OPTIMIZER:
    use_larc: False
    regularize_bn: False
    regularize_bias: True
    construct_single_param_group_only: True
  MODEL:
    FEATURE_EVAL_SETTINGS:
      LINEAR_EVAL_FEAT_POOL_OPS_MAP: [
        # ["res3", ["AdaptiveAvgPool2d", [[2, 2]]]],
        ["res4", ["AdaptiveAvgPool2d", [[2, 1]]]],
        ["res4", ["AdaptiveAvgPool2d", [[1, 2]]]],

        ["res5", ["AdaptiveAvgPool2d", [[1, 2]]]],
        ["res5", ["AdaptiveAvgPool2d", [[2, 1]]]],

        ["avgpool", ["Identity", []]],
        ["avgpool", ["Identity", []]],
      ]
    TRUNK:
      NAME: regnet_fsdp
      REGNET:
        name: anynet
        # depths: [3, 10, 23, 1]  # 10B version of this model
        depths: [1, 4, 4, 1]  # 3B model
        widths: [1840, 3680, 10120, 25760]
        group_widths: [920, 920, 920, 920]
        bottleneck_multipliers: [1.0, 1.0, 1.0, 1.0]
        strides: [2, 2, 2, 2]
        stage_checkpoints: [[1], [4], [4], [1]]
    HEAD:
      PARAMS: [
        # ["eval_mlp_fsdp", {"in_channels": 3680, "dims": [14720, 100]}],

        ["eval_mlp_fsdp", {"in_channels": 10120, "dims": [20240, 100]}],
        ["eval_mlp_fsdp", {"in_channels": 10120, "dims": [20240, 100]}],

        ["eval_mlp_fsdp", {"in_channels": 25760, "dims": [51520, 100]}],
        ["eval_mlp_fsdp", {"in_channels": 25760, "dims": [51520, 100]}],

        ["eval_mlp_fsdp", {"in_channels": 25760, "dims": [25760, 100]}],
        ["mlp_fsdp", {"dims": [25760, 100]}],
      ]
    SYNC_BN_CONFIG:
      CONVERT_BN_TO_SYNC_BN: True
      SYNC_BN_TYPE: pytorch
      GROUP_SIZE: 0  # global sync
    AMP_PARAMS:
      USE_AMP: True
      AMP_TYPE: pytorch
    FSDP_CONFIG:
      # Disabling AUTO_SETUP_FSDP so that:
      # - flatten_parameters is not forced to True
      # - construct_single_param_group_only is not forced to True
      AUTO_SETUP_FSDP: False
      AUTO_WRAP_THRESHOLD: 100000000
      flatten_parameters: False
      compute_dtype: float16
      mixed_precision: True
      fp32_reduce_scatter: False
    CUDA_CACHE:
      CLEAR_CUDA_CACHE: True
      CLEAR_FREQ: 5000
    ACTIVATION_CHECKPOINTING:
      # With linear evaluation, we do not need activation checkpointing
      # because the trunk is used in evaluation mode and does not need
      # to accumulate any gradient
      USE_ACTIVATION_CHECKPOINTING: False
